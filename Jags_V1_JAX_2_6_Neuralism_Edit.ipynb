{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jags V1_JAX 2.6 Neuralism Edit.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "JoIIjmzLNhmc",
        "xbkukxq6d3Qh",
        "WOynujCUCB4M"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jags111/Neuralism-Jax-2.6/blob/main/Jags_V1_JAX_2_6_Neuralism_Edit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZz-fFxNBCug"
      },
      "source": [
        "# Generates images from text prompts with CLIP guided diffusion.\n",
        "\n",
        "Based on my previous jax port of Katherine Crowson's CLIP guided diffusion notebook.\n",
        " - [nshepperd's JAX CLIP Guided Diffusion 512x512.ipynb](https://colab.research.google.com/drive/1ZZi1djM8lU4sorkve3bD6EBHiHs6uNAi)\n",
        " - [CLIP Guided Diffusion HQ 512x512.ipynb](https://colab.research.google.com/drive/1V66mUeJbXrTuQITvJunvnWVn96FEbSI3)\n",
        "\n",
        "Supports both 256x256 and 512x512 OpenAI models.\n",
        "v2.6?:\n",
        " - Added small secondary model for clip guidance.\n",
        " - Added anti-jpeg model for clearer samples.\n",
        " - Added secondary anti-jpeg classifier.\n",
        " - Added Katherine Crowson's v diffusion models (https://github.com/crowsonkb/v-diffusion-jax).\n",
        " - Added pixel art model.\n",
        " - Added cc12m_1 model (https://github.com/crowsonkb/v-diffusion-pytorch)\n",
        " - Reparameterized in terms of cosine t, to allow different schedules; added spliced ddpm+cosine schedule.\n",
        " - Added cc12m_1_cfg model (https://github.com/crowsonkb/v-diffusion-pytorch) and more pixel art models.\n",
        " ---\n",
        "\n",
        "##[NeuralismAI](https://twitter.com/NeuralismAI) edit of nshepperd's JAX 2.6 notebook.\n",
        "\n",
        "This edit consists of forked modification with small changes:\n",
        "- Cleaner Interface (Less raw code)\n",
        "- Prompt Queuing system\n",
        "- Video progress output\n",
        "- Intermediate saves\n",
        "- Diffusion model selection\n",
        "- [Huemin](https://twitter.com/huemin_art)'s simple symmetry\n",
        "\n",
        "[Original notebook](https://colab.research.google.com/drive/1fW_tPEX7iD3xZK3VBDQ_Y2WnfdSzpacM?usp=sharing#scrollTo=zxGgJmRzq3Cs)\n",
        " \n",
        " Any suggestions and feedback do update in Neurlaism discord for same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zKX4uWFBks2"
      },
      "source": [
        "#@title Licensed under the MIT License { display-mode: \"form\" }\n",
        "\n",
        "# Copyright (c) 2021 Katherine Crowson; nshepperd\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKZYWJt087dj",
        "cellView": "form"
      },
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GLI2tEr0AUQ",
        "cellView": "form"
      },
      "source": [
        "#@title Mount Google Drive or save on colab session\n",
        "#@markdown You must run this cell either way.\n",
        "\n",
        "import os # imports here just to prioritize connecting the drive first\n",
        "\n",
        "MOUNT_DRIVE=True #@param {type:\"boolean\"}\n",
        "\n",
        "if MOUNT_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  save_location = '/content/drive/MyDrive/samples/v2'\n",
        "  model_location = '/content/drive/MyDrive/models'\n",
        "  os.makedirs(save_location, exist_ok=True)\n",
        "else:\n",
        "  save_location = \"/content/output_images\"\n",
        "  model_location = 'models'\n",
        "  os.makedirs(save_location, exist_ok=True)\n",
        "\n",
        "os.makedirs(model_location, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial setup:\n",
        "- Install dependencies\n",
        "- Import modules\n",
        "- Define funtions"
      ],
      "metadata": {
        "id": "JoIIjmzLNhmc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ1HpnuQHOpU"
      },
      "source": [
        "# Workaround for https://github.com/googlecolab/colabtools/issues/2452\n",
        "import os\n",
        "if os.system(\"nvidia-smi | grep A100\") == 0:\n",
        "  !pip install -U https://storage.googleapis.com/jax-releases/cuda111/jaxlib-0.1.72+cuda111-cp37-none-manylinux2010_x86_64.whl \"jax==0.1.76\"\n",
        "  # https://storage.googleapis.com/jax-releases/cuda111/jaxlib-0.1.71+cuda111-cp37-none-manylinux2010_x86_64.whl\n",
        "else:\n",
        "  !pip install https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.1.75%2Bcuda11.cudnn805-cp37-none-manylinux2010_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZS4uQYE9BXf"
      },
      "source": [
        "# Install dependencies\n",
        "!pip install dm-haiku==0.0.5 cbor2 ftfy einops braceexpand \n",
        "!git clone https://github.com/nshepperd/CLIP_JAX\n",
        "!git clone https://github.com/nshepperd/jax-guided-diffusion -b v2\n",
        "!git clone https://github.com/crowsonkb/v-diffusion-jax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erek6UjoqR7O"
      },
      "source": [
        "import sys\n",
        "sys.path.append('./CLIP_JAX')\n",
        "sys.path.append('./jax-guided-diffusion')\n",
        "sys.path.append('./v-diffusion-jax')\n",
        "os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'\n",
        "\n",
        "from PIL import Image\n",
        "from braceexpand import braceexpand\n",
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "from subprocess import Popen, PIPE\n",
        "from google.colab import files\n",
        "import functools\n",
        "import io\n",
        "import math\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.scipy as jsp\n",
        "import jaxtorch\n",
        "from jaxtorch import PRNG, Context, Module, nn, init\n",
        "from tqdm import tqdm\n",
        "\n",
        "from lib.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "from lib import util, openai\n",
        "\n",
        "from IPython import display\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torchvision.transforms import functional as TF\n",
        "from subprocess import Popen, PIPE\n",
        "import torch.utils.data\n",
        "import torch\n",
        "\n",
        "import diffusion as v_diffusion\n",
        "\n",
        "from diffusion_models.common import DiffusionOutput, Partial, make_partial, blur_fft, norm1\n",
        "from diffusion_models.cache import WeakCache\n",
        "from diffusion_models.schedules import cosine, ddpm, ddpm2, spliced\n",
        "from diffusion_models.perceptor import vit32, vit16, clip_size, normalize, get_vitl14\n",
        "\n",
        "from diffusion_models.secondary import secondary1_wrap, secondary2_wrap\n",
        "from diffusion_models.antijpeg import jpeg_wrap, jpeg_classifier_wrap\n",
        "from diffusion_models.pixelart import pixelartv4_wrap, pixelartv6_wrap\n",
        "from diffusion_models.pixelartv7 import pixelartv7_ic_wrap, pixelartv7_ic_attn_wrap\n",
        "from diffusion_models.cc12m_1 import cc12m_1_wrap, cc12m_1_cfg_wrap\n",
        "from diffusion_models.openai import make_openai_model, make_openai_finetune_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTWBAsCCqtIz"
      },
      "source": [
        "devices = jax.devices()\n",
        "n_devices = len(devices)\n",
        "print('Using device:', devices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define necessary functions\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def fetch_model(url_or_path):\n",
        "    basename = os.path.basename(url_or_path)\n",
        "    local_path = os.path.join(model_location, basename)\n",
        "    if os.path.exists(local_path):\n",
        "        return local_path\n",
        "    else:\n",
        "        os.makedirs(f'{model_location}/tmp', exist_ok=True)\n",
        "        Popen(['curl', url_or_path, '-o', f'{model_location}/tmp/{basename}']).wait()\n",
        "        os.rename(f'{model_location}/tmp/{basename}', local_path)\n",
        "        return local_path\n",
        "\n",
        "# Implement lazy loading and caching of model parameters for all the different models.\n",
        "\n",
        "gpu_cache = WeakCache(jnp.array)\n",
        "\n",
        "def to_gpu(params):\n",
        "  \"\"\"Convert a pytree of params to jax, using cached arrays if they are still alive.\"\"\"\n",
        "  return jax.tree_util.tree_map(lambda x: gpu_cache(x) if type(x) is np.ndarray else x, params)\n",
        "\n",
        "class LazyParams(object):\n",
        "  \"\"\"Lazily download parameters and load onto gpu. Parameters are kept in cpu memory and only loaded to gpu as long as needed.\"\"\"\n",
        "  def __init__(self, load):\n",
        "    self.load = load\n",
        "    self.params = None\n",
        "  @staticmethod\n",
        "  def pt(url, key=None):\n",
        "    def load():\n",
        "      params = jaxtorch.pt.load(fetch_model(url))\n",
        "      if key is not None:\n",
        "        return params[key]\n",
        "      else:\n",
        "        return params\n",
        "    return LazyParams(load)\n",
        "  def __call__(self):\n",
        "    if self.params is None:\n",
        "      self.params = jax.tree_util.tree_map(np.array, self.load())\n",
        "    return to_gpu(self.params)\n",
        "\n",
        "\n",
        "def grey(image):\n",
        "    [*_, c, h, w] = image.shape\n",
        "    return jnp.broadcast_to(image.mean(axis=-3, keepdims=True), image.shape)\n",
        "\n",
        "def cutout_image(image, offsetx, offsety, size, output_size=224):\n",
        "    \"\"\"Computes (square) cutouts of an image given x and y offsets and size.\"\"\"\n",
        "    (c, h, w) = image.shape\n",
        "\n",
        "    scale = jnp.stack([output_size / size, output_size / size])\n",
        "    translation = jnp.stack([-offsety * output_size / size, -offsetx * output_size / size])\n",
        "    return jax.image.scale_and_translate(image,\n",
        "                                         shape=(c, output_size, output_size),\n",
        "                                         spatial_dims=(1,2),\n",
        "                                         scale=scale,\n",
        "                                         translation=translation,\n",
        "                                         method='lanczos3')\n",
        "\n",
        "def cutouts_images(image, offsetx, offsety, size, output_size=224):\n",
        "    f = partial(cutout_image, output_size=output_size)         # [c h w] [] [] [] -> [c h w]\n",
        "    f = jax.vmap(f, in_axes=(0, None, None, None), out_axes=0) # [n c h w] [] [] [] -> [n c h w]\n",
        "    f = jax.vmap(f, in_axes=(None, 0, 0, 0), out_axes=0)       # [n c h w] [k] [k] [k] -> [k n c h w]\n",
        "    return f(image, offsetx, offsety, size)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MakeCutouts(object):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1., p_grey=0.2, p_mixgrey=0.0):\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.p_grey = p_grey\n",
        "        self.p_mixgrey = p_mixgrey\n",
        "\n",
        "    def __call__(self, input, key):\n",
        "        [b, c, h, w] = input.shape\n",
        "        rng = PRNG(key)\n",
        "        max_size = min(h, w)\n",
        "        min_size = min(h, w, self.cut_size)\n",
        "        cut_us = jax.random.uniform(rng.split(), shape=[self.cutn//2])**self.cut_pow\n",
        "        sizes = (min_size + cut_us * (max_size - min_size + 1)).astype(jnp.int32).clamp(min_size, max_size)\n",
        "        offsets_x = jax.random.uniform(rng.split(), [self.cutn//2], minval=0, maxval=w - sizes)\n",
        "        offsets_y = jax.random.uniform(rng.split(), [self.cutn//2], minval=0, maxval=h - sizes)\n",
        "        cutouts = cutouts_images(input, offsets_x, offsets_y, sizes)\n",
        "\n",
        "        B1 = 40\n",
        "        B2 = 40\n",
        "        lcut_us = jax.random.uniform(rng.split(), shape=[self.cutn//2])\n",
        "        border = B1 + lcut_us * B2\n",
        "        lsizes = (max(h,w) + border).astype(jnp.int32)\n",
        "        loffsets_x = jax.random.uniform(rng.split(), [self.cutn//2], minval=w/2-lsizes/2-border, maxval=w/2-lsizes/2+border)\n",
        "        loffsets_y = jax.random.uniform(rng.split(), [self.cutn//2], minval=h/2-lsizes/2-border, maxval=h/2-lsizes/2+border)\n",
        "        lcutouts = cutouts_images(input, loffsets_x, loffsets_y, lsizes)\n",
        "\n",
        "        cutouts = jnp.concatenate([cutouts, lcutouts], axis=0)\n",
        "\n",
        "        greyed = grey(cutouts)\n",
        "\n",
        "        # Partial greyscale augmentation\n",
        "        grey_us = jax.random.uniform(rng.split(), shape=[self.cutn, b, 1, 1, 1])\n",
        "        grey_rs = jax.random.uniform(rng.split(), shape=[self.cutn, b, 1, 1, 1])\n",
        "        cutouts = jnp.where(grey_us < self.p_mixgrey, grey_rs * greyed + (1 - grey_rs) * cutouts, cutouts)\n",
        "\n",
        "        # Greyscale augmentation\n",
        "        grey_us = jax.random.uniform(rng.split(), shape=[self.cutn, b, 1, 1, 1])\n",
        "        cutouts = jnp.where(grey_us < self.p_grey, greyed, cutouts)\n",
        "\n",
        "        # Flip augmentation\n",
        "        flip_us = jax.random.bernoulli(rng.split(), 0.5, [self.cutn, b, 1, 1, 1])\n",
        "        cutouts = jnp.where(flip_us, jnp.flip(cutouts, axis=-1), cutouts)\n",
        "        return cutouts\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return ([self.p_grey, self.cut_pow, self.p_mixgrey], (self.cut_size, self.cutn))\n",
        "\n",
        "    @staticmethod\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        (cut_size, cutn) = static\n",
        "        (p_grey, cut_pow, p_mixgrey) = dynamic\n",
        "        return MakeCutouts(cut_size, cutn, cut_pow, p_grey, p_mixgrey)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MakeCutoutsPixelated(object):\n",
        "    def __init__(self, make_cutouts, factor=4):\n",
        "        self.make_cutouts = make_cutouts\n",
        "        self.factor = factor\n",
        "        self.cutn = make_cutouts.cutn\n",
        "\n",
        "    def __call__(self, input, key):\n",
        "        [n, c, h, w] = input.shape\n",
        "        input = jax.image.resize(input, [n, c, h*self.factor, w * self.factor], method='nearest')\n",
        "        return self.make_cutouts(input, key)\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return ([self.make_cutouts], [self.factor])\n",
        "    @staticmethod\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return MakeCutoutsPixelated(*dynamic, *static)\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = norm1(x)\n",
        "    y = norm1(y)\n",
        "    return (x - y).square().sum(axis=-1).sqrt().div(2).arcsin().square().mul(2)\n"
      ],
      "metadata": {
        "id": "wVrjUvv0N-9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsLm7ElGc6nQ"
      },
      "source": [
        "# Define combinators.\n",
        "\n",
        "# These (ab)use the jax pytree registration system to define parameterised\n",
        "# objects for doing various things, which are compatible with jax.jit.\n",
        "\n",
        "# For jit compatibility an object needs to act as a pytree, which means implementing two methods:\n",
        "#  - tree_flatten(self): returns two lists of the object's fields:\n",
        "#       1. 'dynamic' parameters: things which can be jax tensors, or other pytrees\n",
        "#       2. 'static' parameters: arbitrary python objects, will trigger recompilation when changed\n",
        "#  - tree_unflatten(static, dynamic): reconstitutes the object from its parts\n",
        "\n",
        "# With these tricks, you can simply define your cond_fn as an object, as is done\n",
        "# below, and pass it into the jitted sample step as a regular argument. JAX will\n",
        "# handle recompiling the jitted code whenever a control-flow affecting parameter\n",
        "# is changed (such as cut_batches).\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class LerpModels(object):\n",
        "    \"\"\"Linear combination of diffusion models.\"\"\"\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "    def __call__(self, x, t, key):\n",
        "        outputs = [m(x,t,key) for (m,w) in self.models]\n",
        "        v = sum(out.v * w for (out, (m,w)) in zip(outputs, self.models))\n",
        "        pred = sum(out.pred * w for (out, (m,w)) in zip(outputs, self.models))\n",
        "        eps = sum(out.eps * w for (out, (m,w)) in zip(outputs, self.models))\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "    def tree_flatten(self):\n",
        "        return [self.models], []\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return LerpModels(*dynamic)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class KatModel(object):\n",
        "    def __init__(self, model, params, **kwargs):\n",
        "      if isinstance(params, LazyParams):\n",
        "        params = params()\n",
        "      self.model = model\n",
        "      self.params = params\n",
        "      self.kwargs = kwargs\n",
        "    @jax.jit\n",
        "    def __call__(self, x, cosine_t, key):\n",
        "        n = x.shape[0]\n",
        "        alpha, sigma = cosine.to_alpha_sigma(cosine_t)\n",
        "        v = self.model.apply(self.params, key, x, cosine_t.broadcast_to([n]), self.kwargs)\n",
        "        pred = x * alpha - v * sigma\n",
        "        eps = x * sigma + v * alpha\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "    def tree_flatten(self):\n",
        "        return [self.params, self.kwargs], [self.model]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [params, kwargs] = dynamic\n",
        "        [model] = static\n",
        "        return KatModel(model, params, **kwargs)\n",
        "\n",
        "# A wrapper that causes the diffusion model to generate tileable images, by\n",
        "# randomly shifting the image with wrap around.\n",
        "\n",
        "def xyroll(x, shifts):\n",
        "  return jax.vmap(partial(jnp.roll, axis=[1,2]), in_axes=(0, 0))(x, shifts)\n",
        "\n",
        "@make_partial\n",
        "def TilingModel(model, x, cosine_t, key):\n",
        "  rng = PRNG(key)\n",
        "  [n, c, h, w] = x.shape\n",
        "  shift = jax.random.randint(rng.split(), [n, 2], -50, 50)\n",
        "  x = xyroll(x, shift)\n",
        "  out = model(x, cosine_t, rng.split())\n",
        "  def unshift(val):\n",
        "    return xyroll(val, -shift)\n",
        "  return jax.tree_util.tree_map(unshift, out)\n",
        "\n",
        "@make_partial\n",
        "def PanoramaModel(model, x, cosine_t, key):\n",
        "  rng = PRNG(key)\n",
        "  [n, c, h, w] = x.shape\n",
        "  shift = jax.random.randint(rng.split(), [n, 2], 0, [1, w])\n",
        "  x = xyroll(x, shift)\n",
        "  out = model(x, cosine_t, rng.split())\n",
        "  def unshift(val):\n",
        "    return xyroll(val, -shift)\n",
        "  return jax.tree_util.tree_map(unshift, out)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a video of the progress after diffusion is complete.\n",
        "!mkdir imagesteps\n",
        "secondsOfVideo = 16\n",
        "\n",
        "\n",
        "def make_video(batchnum):\n",
        "    videoOutputFolder = f\"{save_location}/videos/\"\n",
        "    os.makedirs(videoOutputFolder, exist_ok=True)\n",
        "    timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "    totalFrames = steps\n",
        "    totalFrames -= 1\n",
        "    videoName = f'{all_title}_{batchnum}_{timestring}'\n",
        "    frames = []\n",
        "    fps = 15\n",
        "    if not custom_fps:\n",
        "        fps = steps//secondsOfVideo\n",
        "\n",
        "    tqdm.write(f'Generating video for batch {batchnum}...')\n",
        "    for i in range(totalFrames): \n",
        "        frames.append(Image.open(f\"/content/imagesteps/{batchnum}/\"+str(i)+'.png'))\n",
        "    p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'medium', f'video_{videoName}.mp4'], stdin=PIPE)\n",
        "    for im in tqdm(frames):\n",
        "        im.save(p.stdin, 'PNG')\n",
        "    p.stdin.close()\n",
        "    p.wait()\n",
        "\n",
        "    !rm /content/imagesteps/$batchnum/*.png\n",
        "    \n",
        "    shutil.move(f\"/content/video_{videoName}.mp4\", f\"{videoOutputFolder}video_{videoName}.mp4\")"
      ],
      "metadata": {
        "id": "zT3wzYrmlkgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models & Parameters"
      ],
      "metadata": {
        "id": "xbkukxq6d3Qh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlN36wo_TaFC"
      },
      "source": [
        "# Secondary Model\n",
        "secondary1_params = LazyParams.pt('https://v-diffusion.s3.us-west-2.amazonaws.com/secondary_model_imagenet.pth')\n",
        "secondary2_params = LazyParams.pt('https://v-diffusion.s3.us-west-2.amazonaws.com/secondary_model_imagenet_2.pth')\n",
        "\n",
        "# Anti-JPEG model\n",
        "jpeg_params = LazyParams.pt('https://set.zlkj.in/models/diffusion/jpeg-db-oi-614.pt', key='params_ema')\n",
        "jpeg_classifier_params = LazyParams.pt('https://set.zlkj.in/models/diffusion/jpeg-classifier-72.pt', 'params_ema')\n",
        "\n",
        "# Pixel art model\n",
        "# There are many checkpoints supported with this model\n",
        "pixelartv4_params = LazyParams.pt(\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v4_34.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v4_63.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v4_150.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_50.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_65.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_97.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_173.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_344.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_432.pt'\n",
        "    'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_600.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_700.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_800.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_1000.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_2000.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_3000.pt'\n",
        "    , key='params_ema'\n",
        ")\n",
        "\n",
        "pixelartv6_params = LazyParams.pt(\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-1000.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-2000.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-3000.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-4000.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-aug-900.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-aug-1300.pt'\n",
        "    'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-aug-3000.pt'\n",
        "    , key='params_ema'\n",
        ")\n",
        "\n",
        "pixelartv7_ic_params = LazyParams.pt(\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-ic-1400.pt'\n",
        "    'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v7-large-ic-700.pt'\n",
        "    , key='params_ema'\n",
        ")\n",
        "\n",
        "pixelartv7_ic_attn_params = LazyParams.pt(\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v6-ic-1400.pt'\n",
        "    'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v7-large-ic-attn-600.pt'\n",
        "    , key='params_ema'\n",
        ")\n",
        "\n",
        "# Kat models\n",
        "\n",
        "danbooru_128_model = v_diffusion.get_model('danbooru_128')\n",
        "danbooru_128_params = LazyParams(lambda: v_diffusion.load_params(fetch_model('https://v-diffusion.s3.us-west-2.amazonaws.com/danbooru_128.pkl')))\n",
        "\n",
        "wikiart_256_model = v_diffusion.get_model('wikiart_256')\n",
        "wikiart_256_params = LazyParams(lambda: v_diffusion.load_params(fetch_model('https://v-diffusion.s3.us-west-2.amazonaws.com/wikiart_256.pkl')))\n",
        "\n",
        "wikiart_128_model = v_diffusion.get_model('wikiart_128')\n",
        "wikiart_128_params = LazyParams(lambda: v_diffusion.load_params(fetch_model('https://v-diffusion.s3.us-west-2.amazonaws.com/wikiart_128.pkl')))\n",
        "\n",
        "imagenet_128_model = v_diffusion.get_model('imagenet_128')\n",
        "imagenet_128_params = LazyParams(lambda: v_diffusion.load_params(fetch_model('https://v-diffusion.s3.us-west-2.amazonaws.com/imagenet_128.pkl')))\n",
        "\n",
        "# CC12M_1 model\n",
        "\n",
        "cc12m_1_params = LazyParams.pt('https://v-diffusion.s3.us-west-2.amazonaws.com/cc12m_1.pth')\n",
        "cc12m_1_cfg_params = LazyParams.pt('https://v-diffusion.s3.us-west-2.amazonaws.com/cc12m_1_cfg.pth')\n",
        "\n",
        "# OpenAI models.\n",
        "\n",
        "use_checkpoint = False # Set to True to save some memory\n",
        "\n",
        "openai_512_model = openai.create_openai_512_model(use_checkpoint=use_checkpoint)\n",
        "openai_512_params = openai_512_model.init_weights(jax.random.PRNGKey(0))\n",
        "openai_512_params = LazyParams.pt('https://set.zlkj.in/models/diffusion/512x512_diffusion_uncond_finetune_008100.pt')\n",
        "openai_512_wrap = make_openai_model(openai_512_model)\n",
        "\n",
        "openai_256_model = openai.create_openai_256_model(use_checkpoint=use_checkpoint)\n",
        "openai_256_params = openai_256_model.init_weights(jax.random.PRNGKey(0))\n",
        "openai_256_params = LazyParams.pt('https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt')\n",
        "openai_256_wrap = make_openai_model(openai_256_model)\n",
        "\n",
        "openai_512_finetune_wrap = make_openai_finetune_model(openai_512_model)\n",
        "openai_512_finetune_params = LazyParams.pt('https://set.zlkj.in/models/diffusion/512x512_diffusion_uncond_openimages_epoch28_withfilter.pt')\n",
        "\n",
        "# Aesthetic Model\n",
        "\n",
        "def apply_partial(*args, **kwargs):\n",
        "  def sub(f):\n",
        "    return Partial(f, *args, **kwargs)\n",
        "  return sub\n",
        "\n",
        "aesthetic_model = nn.Linear(512, 10)\n",
        "aesthetic_model.init_weights(jax.random.PRNGKey(0))\n",
        "aesthetic_model_params = jaxtorch.pt.load(fetch_model('https://v-diffusion.s3.us-west-2.amazonaws.com/ava_vit_b_16_full.pth'))\n",
        "\n",
        "def exec_aesthetic_model(params, embed):\n",
        "  return jax.nn.log_softmax(aesthetic_model(Context(params, None), embed), axis=-1)\n",
        "exec_aesthetic_model = Partial(exec_aesthetic_model, aesthetic_model_params)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXQ2Di_LRM46"
      },
      "source": [
        "# Losses and cond fn.\n",
        "\n",
        "@make_partial\n",
        "@apply_partial(exec_aesthetic_model)\n",
        "def AestheticLoss(exec_aesthetic_model, target, scale, image_embeds):\n",
        "    [k, n, d] = image_embeds.shape\n",
        "    log_probs = exec_aesthetic_model(image_embeds)\n",
        "    return -(scale * log_probs[:, :, target-1].mean(0)).sum()\n",
        "\n",
        "@make_partial\n",
        "@apply_partial(exec_aesthetic_model)\n",
        "def AestheticExpected(exec_aesthetic_model, scale, image_embeds):\n",
        "    [k, n, d] = image_embeds.shape\n",
        "    probs = jax.nn.softmax(exec_aesthetic_model(image_embeds))\n",
        "    expected = (probs * (1 + jnp.arange(10))).sum(-1)\n",
        "    return -(scale * expected.mean(0)).sum()\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondCLIP(object):\n",
        "    \"\"\"Backward a loss function through clip.\"\"\"\n",
        "    def __init__(self, perceptor, make_cutouts, cut_batches, *losses):\n",
        "        self.perceptor = perceptor\n",
        "        self.make_cutouts = make_cutouts\n",
        "        self.cut_batches = cut_batches\n",
        "        self.losses = losses\n",
        "    def __call__(self, x_in, key):\n",
        "        n = x_in.shape[0]\n",
        "        def main_clip_loss(x_in, key):\n",
        "            cutouts = normalize(self.make_cutouts(x_in.add(1).div(2), key)).rearrange('k n c h w -> (k n) c h w')\n",
        "            image_embeds = self.perceptor.embed_cutouts(cutouts)\n",
        "            image_embeds = image_embeds.rearrange('(k n) c -> k n c', k=self.make_cutouts.cutn, n=n)\n",
        "            return sum(loss_fn(image_embeds) for loss_fn in self.losses)\n",
        "        num_cuts = self.cut_batches\n",
        "        keys = jnp.stack(jax.random.split(key, num_cuts))\n",
        "        main_clip_grad = jax.lax.scan(lambda total, key: (total + jax.grad(main_clip_loss)(x_in, key), key),\n",
        "                                        jnp.zeros_like(x_in),\n",
        "                                        keys)[0] / num_cuts\n",
        "        return main_clip_grad\n",
        "    def tree_flatten(self):\n",
        "        return [self.perceptor, self.make_cutouts, self.losses], [self.cut_batches]\n",
        "    @classmethod\n",
        "    def tree_unflatten(cls, static, dynamic):\n",
        "        [perceptor, make_cutouts, losses] = dynamic\n",
        "        [cut_batches] = static\n",
        "        return cls(perceptor, make_cutouts, cut_batches, *losses)\n",
        "\n",
        "@make_partial\n",
        "def SphericalDistLoss(text_embed, clip_guidance_scale, image_embeds):\n",
        "    losses = spherical_dist_loss(image_embeds, text_embed).mean(0)\n",
        "    return (clip_guidance_scale * losses).sum()\n",
        "\n",
        "@make_partial\n",
        "def InfoLOOB(text_embed, clip_guidance_scale, inv_tau, lm, image_embeds):\n",
        "    all_image_embeds = norm1(image_embeds.mean(0))\n",
        "    all_text_embeds = norm1(text_embed)\n",
        "    sim_matrix = inv_tau * jnp.einsum('nc,mc->nm', all_image_embeds, all_text_embeds)\n",
        "    xn = sim_matrix.shape[0]\n",
        "    def loob(sim_matrix):\n",
        "      diag = jnp.eye(xn) * sim_matrix\n",
        "      off_diag = (1 - jnp.eye(xn))*sim_matrix + jnp.eye(xn) * float('-inf')\n",
        "      return -diag.sum() + lm * jsp.special.logsumexp(off_diag, axis=-1).sum()\n",
        "    losses = loob(sim_matrix) + loob(sim_matrix.transpose())\n",
        "    return losses.sum() * clip_guidance_scale.mean() / inv_tau\n",
        "\n",
        "@make_partial\n",
        "def CondTV(tv_scale, x_in, key):\n",
        "    def downscale2d(image, f):\n",
        "        [c, n, h, w] = image.shape\n",
        "        return jax.image.resize(image, [c, n, h//f, w//f], method='cubic')\n",
        "\n",
        "    def tv_loss(input):\n",
        "        \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "        x_diff = input[..., :, 1:] - input[..., :, :-1]\n",
        "        y_diff = input[..., 1:, :] - input[..., :-1, :]\n",
        "        return x_diff.square().mean([1,2,3]) + y_diff.square().mean([1,2,3])\n",
        "\n",
        "    def sum_tv_loss(x_in, f=None):\n",
        "        if f is not None:\n",
        "            x_in = downscale2d(x_in, f)\n",
        "        return tv_loss(x_in).sum() * tv_scale\n",
        "    tv_grad_512 = jax.grad(sum_tv_loss)(x_in)\n",
        "    tv_grad_256 = jax.grad(partial(sum_tv_loss,f=2))(x_in)\n",
        "    tv_grad_128 = jax.grad(partial(sum_tv_loss,f=4))(x_in)\n",
        "    return tv_grad_512 + tv_grad_256 + tv_grad_128\n",
        "\n",
        "@make_partial\n",
        "def CondRange(range_scale, x_in, key):\n",
        "    def loss(x_in):\n",
        "        return jnp.abs(x_in - x_in.clamp(minval=-1,maxval=1)).mean()\n",
        "    return range_scale * jax.grad(loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondMSE(target, mse_scale, x_in, key):\n",
        "    def mse_loss(x_in):\n",
        "        return (x_in - target).square().mean()\n",
        "    return mse_scale * jax.grad(mse_loss)(x_in)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MaskedMSE(object):\n",
        "    # MSE loss. Targets the output towards an image.\n",
        "    def __init__(self, target, mse_scale, mask, grey=False):\n",
        "        self.target = target\n",
        "        self.mse_scale = mse_scale\n",
        "        self.mask = mask\n",
        "        self.grey = grey\n",
        "    def __call__(self, x_in, key):\n",
        "        def mse_loss(x_in):\n",
        "            if self.grey:\n",
        "              return (self.mask * grey(x_in - self.target).square()).mean()\n",
        "            else:\n",
        "              return (self.mask * (x_in - self.target).square()).mean()\n",
        "        return self.mse_scale * jax.grad(mse_loss)(x_in)\n",
        "    def tree_flatten(self):\n",
        "        return [self.target, self.mse_scale, self.mask], [self.grey]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return MaskedMSE(*dynamic, *static)\n",
        "\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MainCondFn(object):\n",
        "    # Used to construct the main cond_fn. Accepts a diffusion model which will\n",
        "    # be used for denoising, plus a list of 'conditions' which will\n",
        "    # generate gradient of a loss wrt the denoised, to be summed together.\n",
        "    def __init__(self, diffusion, conditions, blur_amount=None, use='pred'):\n",
        "        self.diffusion = diffusion\n",
        "        self.conditions = [c for c in conditions if c is not None]\n",
        "        self.blur_amount = blur_amount\n",
        "        self.use = use\n",
        "\n",
        "    @jax.jit\n",
        "    def __call__(self, key, x, cosine_t):\n",
        "        rng = PRNG(key)\n",
        "        n = x.shape[0]\n",
        "\n",
        "        alphas, sigmas = cosine.to_alpha_sigma(cosine_t)\n",
        "\n",
        "        def denoise(key, x):\n",
        "            pred = self.diffusion(x, cosine_t, key).pred\n",
        "            if self.use == 'pred':\n",
        "                return pred\n",
        "            elif self.use == 'x_in':\n",
        "                return pred * sigmas + x * alphas\n",
        "        (x_in, backward) = jax.vjp(partial(denoise, rng.split()), x)\n",
        "\n",
        "        total = jnp.zeros_like(x_in)\n",
        "        for cond in self.conditions:\n",
        "            total += cond(x_in, rng.split())\n",
        "        if self.blur_amount is not None:\n",
        "          blur_radius = (self.blur_amount * sigmas / alphas).clamp(0.05,512)\n",
        "          total = blur_fft(total, blur_radius.mean())\n",
        "        final_grad = -backward(total)[0]\n",
        "\n",
        "        # clamp gradients to a max of 0.2\n",
        "        magnitude = final_grad.square().mean(axis=(1,2,3), keepdims=True).sqrt()\n",
        "        final_grad = final_grad * jnp.where(magnitude > 0.2, 0.2 / magnitude, 1.0)\n",
        "        return final_grad\n",
        "    def tree_flatten(self):\n",
        "        return [self.diffusion, self.conditions, self.blur_amount], [self.use]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return MainCondFn(*dynamic, *static)\n",
        "\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondFns(object):\n",
        "    def __init__(self, *conditions):\n",
        "        self.conditions = conditions\n",
        "    def __call__(self, key, x, t):\n",
        "        rng = PRNG(key)\n",
        "        total = jnp.zeros_like(x)\n",
        "        for cond in self.conditions:\n",
        "          total += cond(rng.split(), x, t)\n",
        "        return total\n",
        "    def tree_flatten(self):\n",
        "        return [self.conditions], []\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [conditions] = dynamic\n",
        "        return CondFns(*conditions)\n",
        "\n",
        "def clamp_score(score):\n",
        "  magnitude = score.square().mean(axis=(1,2,3), keepdims=True).sqrt()\n",
        "  return score * jnp.where(magnitude > 0.1, 0.1 / magnitude, 1.0)\n",
        "\n",
        "\n",
        "@make_partial\n",
        "def BlurRangeLoss(scale, key, x, cosine_t):\n",
        "    def blurred_pred(x, cosine_t):\n",
        "      alpha, sigma = cosine.to_alpha_sigma(cosine_t)\n",
        "      blur_radius = (sigma / alpha * 2).clamp(0.05,512)\n",
        "      return blur_fft(x, blur_radius) / alpha.clamp(0.01)\n",
        "    def loss(x):\n",
        "        pred = blurred_pred(x, cosine_t)\n",
        "        diff = pred - pred.clamp(minval=-1,maxval=1)\n",
        "        return diff.square().sum()\n",
        "    return clamp_score(-scale * jax.grad(loss)(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBlHR4k2lfiB"
      },
      "source": [
        "def sample_step(key, x, t1, t2, diffusion, cond_fn, eta):\n",
        "    rng = PRNG(key)\n",
        "\n",
        "    n = x.shape[0]\n",
        "    alpha1, sigma1 = cosine.to_alpha_sigma(t1)\n",
        "    alpha2, sigma2 = cosine.to_alpha_sigma(t2)\n",
        "\n",
        "    # Run the model\n",
        "    out = diffusion(x, t1, rng.split())\n",
        "    eps = out.eps\n",
        "    pred0 = out.pred\n",
        "\n",
        "    # # Predict the denoised image\n",
        "    # pred0 = (x - eps * sigma1) / alpha1\n",
        "\n",
        "    # Adjust eps with conditioning gradient\n",
        "    cond_score = cond_fn(rng.split(), x, t1)\n",
        "    eps = eps - sigma1 * cond_score\n",
        "\n",
        "    # Predict the denoised image with conditioning\n",
        "    pred = (x - eps * sigma1) / alpha1\n",
        "\n",
        "    # Negative eta allows more extreme levels of noise.\n",
        "    ddpm_sigma = (sigma2**2 / sigma1**2).sqrt() * (1 - alpha1**2 / alpha2**2).sqrt()\n",
        "    ddim_sigma = jnp.where(eta >= 0.0,\n",
        "                           eta * ddpm_sigma, # Normal: eta interpolates between ddim and ddpm\n",
        "                           -eta * sigma2)    # Extreme: eta interpolates between ddim and q_sample(pred)\n",
        "    adjusted_sigma = (sigma2**2 - ddim_sigma**2).sqrt()\n",
        "\n",
        "    # Recombine the predicted noise and predicted denoised image in the\n",
        "    # correct proportions for the next step\n",
        "    x = pred * alpha2 + eps * adjusted_sigma\n",
        "\n",
        "    # Add the correct amount of fresh noise\n",
        "    x += jax.random.normal(rng.split(), x.shape) * ddim_sigma\n",
        "    return x, pred0\n",
        "\n",
        "def process_prompt(clip, prompt):\n",
        "  # Brace expansion might change later, not sure this is the best way to do it.\n",
        "  expands = braceexpand(prompt)\n",
        "  embeds = []\n",
        "  for sub in expands:\n",
        "    mult = 1.0\n",
        "    if '~' in sub:\n",
        "      mult *= -1.0\n",
        "    sub = sub.replace('~', '')\n",
        "    embeds.append(mult * clip.embed_text(sub))\n",
        "  return norm1(sum(embeds))\n",
        "\n",
        "def process_prompts(clip, prompts):\n",
        "  return jnp.stack([process_prompt(clip, prompt) for prompt in prompts])\n",
        "\n",
        "def expand(xs, batch_size):\n",
        "  \"\"\"Extend or truncate the list of prompts to the batch size.\"\"\"\n",
        "  return (xs * batch_size)[:batch_size]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOynujCUCB4M"
      },
      "source": [
        "# Configuration for the run (tutorial)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Some paramaters are not listed here, simply because their functionality is obvious.\n",
        "\n",
        "- `seed`: Allows reproductability of an image. `None` is for a random seed.\n",
        "- `use_model`: which diffusion model to use (the usual model in JAX is `openai`)\n",
        "- `batch_size`: how many images to render next to eachother\n",
        "- `n_batches`: how many times to run the current prompt\n",
        "- `clip_guidance_scale`: how much the image should look like the prompt\n",
        "- `cfg_guidance_scale`: same as `clip_guidance_scale` but for cc12m_cfg\n",
        "- `tv_scale`: controls the smoothness of the image\n",
        "- `sat_scale`: controls the saturation of the image\n",
        "- `cutn`: amount of cuts clip does\n",
        "- `cut_batches` -> multiplier for `cutn` (`cutn` * `cut_batches`) \n",
        "- `intermediate_step_saves:` Saves different steps of the current rendering image. \n",
        "example: `[50, 225]` will save an image of step 50 and 225. It will always save the final interation of the image with `_final` at the end of the image name. Leave this paramater as `[]` to not save any intermediate steps.\n",
        "- `secondsOfVideo`: set a fixed length of the progress video, fps will be automatically calculated.\n",
        "- With `custom_fps` enabled, `fps` is a fixed frame speed for the progress video. The video length will be `steps / fps`\n",
        "\n",
        "\n",
        "\n",
        "The batch settings will apply individually to each prompt in the queue.\n"
      ],
      "metadata": {
        "id": "u32DvJFGgNKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration for the run"
      ],
      "metadata": {
        "id": "mrjPCA8T0HWe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxGgJmRzq3Cs",
        "cellView": "form"
      },
      "source": [
        "image_size = (640, 512) #@param {type:\"raw\"}\n",
        "batch_size = 1 #@param {type:\"integer\"}\n",
        "n_batches =  1#@param {type:\"integer\"}\n",
        "use_model = 'openai' #@param [\"openai\", \"openai_finetune\",  \"cc12m_1\", \"cc12m_1_cfg\", \"wikiart_256\", \"wikiart_128\", \"danbooru_128\", \"imagenet_128\", \"pixelartv4\", \"pixelartv6\", \"pixelartv7_ic_attn\"]\n",
        "steps = 250 #@param {type:\"integer\"}    # Number of steps for sampling. Generally, more = better.\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown Separate prompts to queue by a ``|``\n",
        "prompts = \"sharp cyberpunk robots :1 | Pop art bold pastel colors : 1 |  strong sharp  twisted  | Max Ernst  | trending in artstation:1 |metallic:-0.1\" #@param {type:\"string\"}\n",
        "prompts = [prompt.strip() for prompt in prompts.split(\"|\") if prompt != \"\"]\n",
        "switch_seed_per_prompt = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "\n",
        "ic_cond = 'https://irc.zlkj.in/uploads/eebeaf1803e898ac/88552154_p0%20-%20Coral.png'\n",
        "# 'https://irc.zlkj.in/uploads/eebeaf1803e898ac/88552154_p0%20-%20Coral.png'\n",
        "# 'https://cdn.discordapp.com/emojis/916943952597360690.png?size=240&quality=lossless' # pizagal\n",
        "\n",
        "clip_guidance_scale =  40000#@param {type:\"integer\"}\n",
        "clip_guidance_scale = jnp.array([clip_guidance_scale]*batch_size) # Note: with two perceptors, effective guidance scale is ~2x because they are added together.\n",
        "cfg_guidance_scale = 2.0  #@param {type:\"number\"}\n",
        "tv_scale =   150#@param {type:\"integer\"}# Smooths out the image\n",
        "range_scale =  600#@param {type:\"integer\"}# Tries to prevent pixel values from going out of range\n",
        "cutn =         8#@param {type:\"integer\"}# Effective cutn is cut_batches * this\n",
        "cut_pow = 1.0   # Affects the size of cutouts. Larger cut_pow -> smaller cutouts (down to the min of 224x244)\n",
        "cut_batches = 4 #@param {type:\"integer\"}\n",
        "make_cutouts = MakeCutouts(clip_size, cutn, cut_pow=cut_pow, p_mixgrey=0.0)\n",
        "\n",
        "\n",
        "eta = 1.0       # 0.0: DDIM | 1.0: DDPM | -1.0: Extreme noise (q_sample)\n",
        "#@markdown ---\n",
        "init_image = \"/content/drive/MyDrive/AI/nshepv2/init/robotLarge01A.png\"    #@param {type:\"string\"}\n",
        "starting_noise = 1.0  #@param {type:\"number\"} # Between 0 and 1. When using init image, generally 0.5-0.8 is good. Lower starting noise makes the result look more like the init.\n",
        "init_weight_mse = 0    # MSE loss between the output and the init makes the result look more like the init (should be between 0 and width*height*3).\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "saveVideo = False #@param {type:\"boolean\"}\n",
        "#@markdown For automatic FPS:\n",
        "secondsOfVideo =  7#@param {type:\"number\"}\n",
        "#@markdown For manual FPS:\n",
        "custom_fps = False #@param {type:\"boolean\"}\n",
        "fps =  14#@param {type:\"number\"}\n",
        "\n",
        "\n",
        "\n",
        "for k in range(batch_size): # creates folder that will hold steps for video\n",
        "        imagestepsFolder = f'/content/imagesteps/{k}'\n",
        "        os.makedirs(imagestepsFolder, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "schedule = jnp.linspace(starting_noise, 0, steps+1)\n",
        "schedule = spliced.to_cosine(schedule)\n",
        "\n",
        "if init_image == \"None\": init_image = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_image(url):\n",
        "    init_array = Image.open(fetch(url)).convert('RGB')\n",
        "    init_array = init_array.resize(image_size, Image.LANCZOS)\n",
        "    init_array = jnp.array(TF.to_tensor(init_array)).unsqueeze(0).mul(2).sub(1)\n",
        "    return init_array\n",
        "if type(init_image) is str:\n",
        "    init_array = jnp.concatenate([load_image(it) for it in braceexpand(init_image)], axis=0)\n",
        "else:\n",
        "    init_array = None\n",
        "\n",
        "def config():\n",
        "    # Configure models and load parameters onto gpu.\n",
        "    # We do this in a function to avoid leaking gpu memory.\n",
        "    if use_model == 'openai':\n",
        "        # -- Openai with anti-jpeg --\n",
        "        openai = openai_512_wrap(openai_512_params())\n",
        "        secondary2 = secondary2_wrap(secondary2_params())\n",
        "        jpeg_0 = jpeg_wrap(jpeg_params(), cond=jnp.array([0]*batch_size)) # Clean class\n",
        "        jpeg_1 = jpeg_wrap(jpeg_params(), cond=jnp.array([2]*batch_size)) # Unconditional class\n",
        "\n",
        "        \n",
        "        jpeg_classifier_fn = jpeg_classifier_wrap(jpeg_classifier_params(),\n",
        "                                                  guidance_scale=10000.0, # will generally depend on image size\n",
        "                                                  flood_level=0.7, # Prevent over-optimization\n",
        "                                                  blur_size=3.0)\n",
        "        \n",
        "\n",
        "        diffusion = LerpModels([(openai, 1.0),\n",
        "                                (jpeg_0, 1.0),\n",
        "                                (jpeg_1, -1.0)])\n",
        "        cond_model = secondary2\n",
        "\n",
        "        cond_fn = CondFns(MainCondFn(cond_model, [\n",
        "            CondCLIP(vit32, make_cutouts, cut_batches, SphericalDistLoss(process_prompts(vit32, title), clip_guidance_scale)),\n",
        "            CondCLIP(vit16, make_cutouts, cut_batches, SphericalDistLoss(process_prompts(vit16, title), clip_guidance_scale)),\n",
        "            CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "            CondMSE(init_array, init_weight_mse) if init_weight_mse > 0 else None,\n",
        "            CondRange(range_scale) if range_scale > 0 else None,\n",
        "        ]), jpeg_classifier_fn)\n",
        "\n",
        "    elif use_model in ('wikiart_256', 'wikiart_128', 'danbooru_128', 'imagenet_128'):\n",
        "        if use_model == 'wikiart_256':\n",
        "            diffusion = KatModel(wikiart_256_model, wikiart_256_params())\n",
        "        elif use_model == 'wikiart_128':\n",
        "            diffusion = KatModel(wikiart_128_model, wikiart_128_params())\n",
        "        elif use_model == 'danbooru_128':\n",
        "            diffusion = KatModel(danbooru_128_model, danbooru_128_params())\n",
        "        elif use_model == 'imagenet_128':\n",
        "            diffusion = KatModel(imagenet_128_model, imagenet_128_params())\n",
        "        cond_model = diffusion\n",
        "        cond_fn = MainCondFn(cond_model, [\n",
        "                    CondCLIP(vit32, make_cutouts, cut_batches,\n",
        "                             SphericalDistLoss(process_prompts(vit32, title), clip_guidance_scale)),\n",
        "                    CondCLIP(vit16, make_cutouts, cut_batches,\n",
        "                             SphericalDistLoss(process_prompts(vit16, title), clip_guidance_scale)),\n",
        "                    CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "                    CondMSE(init_array, init_weight_mse) if init_weight_mse > 0 else None,\n",
        "                    CondRange(range_scale) if range_scale > 0 else None,\n",
        "                    ])\n",
        "\n",
        "    elif 'pixelart' in use_model:\n",
        "        if use_model == 'pixelartv7_ic_attn':\n",
        "            # -- pixel art model --\n",
        "            cond = jnp.array(TF.to_tensor(Image.open(fetch(ic_cond)).convert('RGB').resize(image_size,Image.BICUBIC))) * 2 - 1\n",
        "            cond = cond.broadcast_to([batch_size, 3, image_size[1], image_size[0]])\n",
        "            diffusion = pixelartv7_ic_attn_wrap(pixelartv7_ic_attn_params(), cond=cond, cfg_guidance_scale=cfg_guidance_scale)\n",
        "        elif use_model == 'pixelartv6':\n",
        "            diffusion = pixelartv6_wrap(pixelartv6_params())\n",
        "        elif use_model == 'pixelartv4':\n",
        "            diffusion = pixelartv4_wrap(pixelartv4_params())\n",
        "        cond_model = diffusion\n",
        "        cond_fn = MainCondFn(cond_model, [\n",
        "            CondCLIP(vit32, MakeCutoutsPixelated(make_cutouts), cut_batches,\n",
        "                     SphericalDistLoss(process_prompts(vit32, title), clip_guidance_scale)),\n",
        "            CondCLIP(vit16, MakeCutoutsPixelated(make_cutouts), cut_batches,\n",
        "                     SphericalDistLoss(process_prompts(vit16, title), clip_guidance_scale)),\n",
        "            CondMSE(init_array, init_weight_mse) if init_weight_mse > 0 else None,\n",
        "        ])\n",
        "\n",
        "    elif use_model == 'cc12m_1_cfg':\n",
        "        diffusion = cc12m_1_cfg_wrap(cc12m_1_cfg_params(), clip_embed=vit16.embed_texts(title), cfg_guidance_scale=cfg_guidance_scale)\n",
        "        cond_fn = CondFns()\n",
        "    \n",
        "    elif use_model == 'cc12m_1':\n",
        "        diffusion = cc12m_1_wrap(cc12m_1_params(), clip_embed=vit16.embed_texts(title))\n",
        "        cond_model = diffusion\n",
        "        cond_fn = MainCondFn(cond_model, [\n",
        "                    CondCLIP(vit32, make_cutouts, cut_batches, SphericalDistLoss(process_prompts(vit32, title), clip_guidance_scale)),\n",
        "                    CondCLIP(vit16, make_cutouts, cut_batches, SphericalDistLoss(process_prompts(vit16, title), clip_guidance_scale)),\n",
        "                    CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "                    CondMSE(init_array, init_weight_mse) if init_weight_mse > 0 else None,\n",
        "                    CondRange(range_scale) if range_scale > 0 else None,\n",
        "                    ])\n",
        "\n",
        "    elif use_model == 'openai_finetune':\n",
        "        diffusion = openai_512_finetune_wrap(openai_512_finetune_params())\n",
        "        cond_model = secondary2_wrap(secondary2_params())\n",
        "        cond_fn = CondFns(MainCondFn(cond_model, [\n",
        "                    CondCLIP(vit32, make_cutouts, cut_batches, SphericalDistLoss(process_prompts(vit32, title), clip_guidance_scale)),\n",
        "                    CondCLIP(vit16, make_cutouts, cut_batches,\n",
        "                              SphericalDistLoss(process_prompts(vit16, title), clip_guidance_scale),\n",
        "                              AestheticExpected(jnp.array([16.0,16.0,16.0,16.0]))\n",
        "                             ),\n",
        "                    CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "                    CondMSE(init_array, init_weight_mse) if init_weight_mse > 0 else None,\n",
        "                    CondRange(range_scale) if range_scale > 0 else None,\n",
        "                    ]))\n",
        "\n",
        "    return diffusion, cond_fn\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Huemin's simple symmetry\n",
        "#@markdown `symmetry_schedule`: percentage values of when to perform the symmetry. To perform it multilpe times, separate the values by a comma. \n",
        "def simple_symmetry(x_in):\n",
        "  [n, c, h, w] = x_in.shape\n",
        "  x_in = jnp.concatenate([x_in[:, :, :, :w//2], jnp.flip(x_in[:, :, :, :w//2],-1)], -1)\n",
        "  return(x_in)\n",
        "\n",
        "#@markdown Symmetry Settings\n",
        "use_symmetry = True #@param {type:\"boolean\"}\n",
        "symmetry_schedule = \"0.01,0.5\"#@param {type:\"string\"}\n",
        "\n",
        "symmetry_percents = [float(vals) for vals in symmetry_schedule.split(\",\")]"
      ],
      "metadata": {
        "id": "HqKgKxNieY7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Do the run"
      ],
      "metadata": {
        "id": "CCAVCShneT52"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoIL7ayzq7kC",
        "cellView": "form"
      },
      "source": [
        "#@markdown If `switch_seed_per_prompt` is enabled, `seed` will be a random number, even if you set it to a specific number.\n",
        "\n",
        "seed = None #@param # if None, uses the current time in seconds.\n",
        "display_frequency = 50 #@param {type:\"number\"}\n",
        "intermediate_saves = 100,200 #@param\n",
        "\n",
        "def sanitize(title):\n",
        "  return title[:100].replace('/', '_').replace('\\\\', '_')\n",
        "\n",
        "@torch.no_grad()\n",
        "def run():\n",
        "\n",
        "    rng = PRNG(jax.random.PRNGKey(local_seed))\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "\n",
        "        ts = schedule\n",
        "        alphas, sigmas = cosine.to_alpha_sigma(ts)\n",
        "\n",
        "        #print(ts[0], sigmas[0], alphas[0])\n",
        "\n",
        "        x = jax.random.normal(rng.split(), [batch_size, 3, image_size[1], image_size[0]])\n",
        "\n",
        "        if init_array is not None:\n",
        "            x = sigmas[0] * x + alphas[0] * init_array\n",
        "\n",
        "        # Creates the ./images folder in advance for intermediate image saves\n",
        "        os.makedirs('samples/images', exist_ok=True)\n",
        "        if save_location:\n",
        "          os.makedirs(f'{save_location}/images', exist_ok=True)\n",
        "\n",
        "        # Main loop\n",
        "        local_steps = schedule.shape[0] - 1\n",
        "        for j in tqdm(range(local_steps)):\n",
        "            # == Panorama ==\n",
        "            # shift = jax.random.randint(rng.split(), [batch_size, 2], 0, jnp.array([1, image_size[0]]))\n",
        "            # x = xyroll(x, shift) \n",
        "            # == -------- ==\n",
        "            if ts[j] == ts[j+1]:\n",
        "              continue\n",
        "            # Skip steps where the ts are the same, to make it easier to\n",
        "            # make complicated schedules out of cat'ing linspaces.\n",
        "            # diffusion.set(clip_embed=jax.random.normal(rng.split(), [batch_size,512]))\n",
        "            \n",
        "            # performs the symmetry\n",
        "            if (use_symmetry) and (j in symmetry_steps) and (j != 0):\n",
        "              x = simple_symmetry(x)\n",
        "              print(\"simple symmetry\")\n",
        "            \n",
        "            x, pred = sample_step(rng.split(), x, ts[j], ts[j+1], diffusion, cond_fn, eta)\n",
        "            assert x.isfinite().all().item()\n",
        "            if j % display_frequency == 0 or j == local_steps - 1:\n",
        "                images = pred\n",
        "                # images = jnp.concatenate([images, x], axis=0)\n",
        "                images = images.add(1).div(2).clamp(0, 1)\n",
        "                images = torch.tensor(np.array(images))\n",
        "                display.display(TF.to_pil_image(utils.make_grid(images, 4).cpu()))\n",
        "            \n",
        "            if j in intermediate_saves: # saves itermediate steps\n",
        "                for k in range(batch_size):\n",
        "                    images = pred\n",
        "                    this_title = sanitize(title[k])\n",
        "                    dname = f'samples/images/{this_title}_{k}_{timestring}_{j}.png'\n",
        "                    images = images.add(1).div(2).clamp(0, 1)\n",
        "                    images = torch.tensor(np.array(images))\n",
        "                    pil_image = TF.to_pil_image(utils.make_grid(images, 2).cpu())\n",
        "                    print(f\" [Intermediate Save]\")\n",
        "                    display.display(pil_image)\n",
        "                    pil_image.save(dname)\n",
        "                    if save_location:\n",
        "                        pil_image.save(f'{save_location}/images/{this_title}_{k}_{timestring}_{j}.png')\n",
        "\n",
        "            \n",
        "            if saveVideo:\n",
        "                for k in range(batch_size):\n",
        "                    images = pred.add(1).div(2).clamp(0, 1)\n",
        "                    images = torch.tensor(np.array(images))\n",
        "                    stepnum = f'{j}.png' \n",
        "                    pil_image = TF.to_pil_image(images[k])\n",
        "                    pil_image.save(f'/content/imagesteps/{k}/'+stepnum)\n",
        "            \n",
        "\n",
        "        # Save samples\n",
        "        os.makedirs('samples/grid', exist_ok=True)\n",
        "        if save_location:\n",
        "          os.makedirs(f'{save_location}/grid', exist_ok=True)\n",
        "        TF.to_pil_image(utils.make_grid(images, 2).cpu()).save(f'samples/grid/{timestring}_{sanitize(all_title)}.png')\n",
        "        TF.to_pil_image(utils.make_grid(images, 2).cpu()).save(f'{save_location}/grid/{timestring}_{sanitize(all_title)}.png')\n",
        "\n",
        "        for k in range(batch_size):\n",
        "            this_title = sanitize(title[k])\n",
        "            dname = f'samples/images/{this_title}_{k}_{timestring}_final.png'\n",
        "            pil_image = TF.to_pil_image(images[k])\n",
        "            pil_image.save(dname)\n",
        "            if save_location:\n",
        "              pil_image.save(f'{save_location}/images/{this_title}_{k}_{timestring}_final.png')\n",
        "            if saveVideo:\n",
        "                make_video(k)\n",
        "\n",
        "\n",
        "# so that there can be the same seed for each prompt\n",
        "if (seed == None) and (not switch_seed_per_prompt):\n",
        "        seed = int(time.time())\n",
        "\n",
        "\n",
        "\n",
        "for all_title in prompts:\n",
        "    \n",
        "    title = expand([all_title], batch_size)\n",
        "\n",
        "    # must be moved here since the locations of all_title and title are in this loop\n",
        "    diffusion, cond_fn = config() \n",
        "\n",
        "\n",
        "    # Weird seeding to account for different options\n",
        "    if (switch_seed_per_prompt) or (seed == None): \n",
        "        local_seed = int(time.time())\n",
        "        \n",
        "\n",
        "    elif (not switch_seed_per_prompt) or (seed != None):\n",
        "        local_seed = seed\n",
        "        \n",
        "\n",
        "    # sets when to perform the symmetry\n",
        "    if use_symmetry:\n",
        "        symmetry_steps = [int(steps*percent) for percent in symmetry_percents]\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Current prompt: {all_title}\")\n",
        "    print(f\"Current seed: {local_seed}\")\n",
        "\n",
        "    try:\n",
        "        run()\n",
        "        success = True\n",
        "    except:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        success = False\n",
        "    assert success\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title zip and download all of the output images\n",
        "\n",
        "make_zip = False #@param {type:\"boolean\"}\n",
        "\n",
        "zipName = \"JaxZipOutput.zip\"\n",
        "\n",
        "if make_zip:  \n",
        "    if os.path.exists(zipName):\n",
        "        os.remove(zipName)\n",
        "    #os.system(f\"zip -r -j {zipName} results/*\")\n",
        "    !zip -r {zipName} {save_location}/images\n",
        "    files.download(zipName)\n"
      ],
      "metadata": {
        "id": "I6f0JD84f-oA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}